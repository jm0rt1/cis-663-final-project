Ensuring an equal balance of classes during a train-test split requires stratified sampling. The idea behind stratified sampling is to divide the entire dataset into homogeneous subgroups called strata, and then samples are taken proportionally from these strata.

In the context of train-test splits for machine learning, this means that the proportions of each class in the dataset are preserved in both the training and testing sets.

Here's how you can achieve a 50-50 split for your classes using `train_test_split` from Scikit-Learn:

1. **Prepare Your Data**:
   Before you split, ensure that your dataset is structured such that you have the features (images) in one array/list and the labels ("you" or "not you") in another.

2. **Use Stratified Sampling**:
   The `train_test_split` function has a parameter called `stratify`. If you set this parameter to the labels, it will ensure that the train and test sets have approximately the same percentage of samples of each target class as the complete set.

Here's a simple example:

``` python
from sklearn.model_selection import train_test_split

# X is your feature set (e.g., images)
# y is your labels set (e.g., "you" or "not you")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
```

In this example, 20% of the data goes to the test set, and because we set `stratify=y`, the proportion of classes in `y_train` and `y_test` will be approximately the same as in `y`.

To ensure that your dataset has 50% "you" images:

1. You might have to either collect more "you" images or undersample the "not you" images.
2. After ensuring your entire dataset has a 50-50 split, use stratified sampling (as shown above) to ensure the train and test sets also respect this distribution.

Remember, always set a `random_state` if you want to ensure reproducibility in your experiments.



## Stratified Sampling made things worse
dding the stratify actually killed the results, why might that be?

Classification Report #1 -- Percentage of Target in Dataset: 50
SMOTE Resampled = True

              precision    recall  f1-score   support

     Not You       0.00      0.00      0.00         4
         You       0.50      1.00      0.67         4

    accuracy                           0.50         8
   macro avg       0.25      0.50      0.33         8
weighted avg       0.25      0.50      0.33         8


## Suggestions

If adding stratification worsened the results, there could be a few reasons for it:

1. **Small Dataset**: If your dataset is small (as it seems from the results, with just 8 samples in your test set), it's more likely to observe variances in performance due to small changes in the dataset. This makes the model more susceptible to statistical noise, and the performance might vary significantly across different train-test splits.

2. **Overfitting**: Stratifying ensures that the class distribution in the training set matches the overall dataset. In cases where the dataset is imbalanced, stratification can help. But if your model was somehow "taking advantage" of certain patterns that existed due to a non-stratified split and performing well on the test set because of that, stratification can break that and expose the model's inability to generalize.

3. **Resampling Effects**: You're using SMOTE, which is a synthetic oversampling technique. SMOTE generates synthetic samples in the feature space. When stratification is applied, the training data distribution changes, and thus the synthetic samples generated by SMOTE could be different than without stratification. It's possible that these synthetic samples are not helping the model, or even making it worse.

4. **Inherent Variability**: Machine learning models, especially on small datasets, can show a wide range of performances based on different splits or minor changes. It's possible that the specific stratified split was just particularly challenging or not representative for the model.

5. **Evaluation Metric**: Accuracy is highly sensitive to class balance. Given that your dataset is now balanced (50% "You" and 50% "Not You"), a model guessing randomly would get 50% accuracy. It might be beneficial to look at other metrics like F1-Score, Precision, Recall, or the ROC AUC score to evaluate performance in a more nuanced way.

To help further:

1. **Increase Dataset Size**: If possible, gather more data. Small datasets are inherently noisy, and results can vary greatly with minor changes.
   
2. **Cross-Validation**: Instead of a single train-test split, use cross-validation. This will give you a better understanding of the expected performance and its variance.

3. **Tune SMOTE**: Try adjusting the parameters of SMOTE or even try without SMOTE since your data is already balanced.

4. **Model Complexity**: If your model is too complex, it might overfit to the training data, especially with small datasets. Try simpler models or regularization techniques.

Finally, always remember that machine learning models are data-driven. The quality and quantity of the data play a major role in the performance of the model. If the dataset is small or if it's not representative of the real-world distribution, even the best models can perform poorly.

## Updates made to the "You" dataset

found 75 images of myself to replace the ~15 images of Tom Cruise
placed in the